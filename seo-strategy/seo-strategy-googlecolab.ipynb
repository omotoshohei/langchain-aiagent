{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyPyILOboOrJaYLQ5YU3cPrx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install langchain==0.3.0 langchain-openai==0.2.0 langgraph==0.2.22 httpx==0.27.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z0qYIfSkXLuC","executionInfo":{"status":"ok","timestamp":1736130145791,"user_tz":-540,"elapsed":15478,"user":{"displayName":"Shohei O","userId":"12040440607234237913"}},"outputId":"601c53f8-4f75-45d6-fc71-0a01853653b3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain==0.3.0\n","  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-openai==0.2.0\n","  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting langgraph==0.2.22\n","  Downloading langgraph-0.2.22-py3-none-any.whl.metadata (13 kB)\n","Collecting httpx==0.27.2\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (6.0.2)\n","Collecting SQLAlchemy<3,>=1.4 (from langchain==0.3.0)\n","  Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.3.0)\n","  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Collecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.3.0)\n","  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n","Collecting langchain-core<0.4.0,>=0.3.0 (from langchain==0.3.0)\n","  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n","Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.0)\n","  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.0)\n","  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.10.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.32.3)\n","Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.3.0)\n","  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting openai<2.0.0,>=1.40.0 (from langchain-openai==0.2.0)\n","  Downloading openai-1.59.3-py3-none-any.whl.metadata (27 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.0)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting langgraph-checkpoint<2.0.0,>=1.0.2 (from langgraph==0.2.22)\n","  Downloading langgraph_checkpoint-1.0.12-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (2024.12.14)\n","Collecting httpcore==1.* (from httpx==0.27.2)\n","  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx==0.27.2)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n","  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n","  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (24.3.0)\n","Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n","  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n","  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n","Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n","  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n","Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n","  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (4.12.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<2.0.0,>=1.0.2->langgraph==0.2.22) (1.1.0)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0)\n","  Downloading orjson-3.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.7.0)\n","Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0)\n","  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (4.67.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx==0.27.2) (1.2.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (2.2.3)\n","Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.3.0)\n","  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.0) (2024.11.6)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain==0.3.0)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n","Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph-0.2.22-py3-none-any.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n","Downloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\n","Downloading langgraph_checkpoint-1.0.12-py3-none-any.whl (17 kB)\n","Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.59.3-py3-none-any.whl (454 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n","Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n","Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n","Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orjson-3.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Installing collected packages: tenacity, propcache, orjson, multidict, jsonpointer, jiter, h11, greenlet, frozenlist, async-timeout, aiohappyeyeballs, yarl, tiktoken, SQLAlchemy, requests-toolbelt, jsonpatch, httpcore, aiosignal, httpx, aiohttp, openai, langsmith, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph, langchain\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","Successfully installed SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 async-timeout-4.0.3 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.27.2 jiter-0.8.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-core-0.3.29 langchain-openai-0.2.0 langchain-text-splitters-0.3.4 langgraph-0.2.22 langgraph-checkpoint-1.0.12 langsmith-0.1.147 multidict-6.1.0 openai-1.59.3 orjson-3.10.13 propcache-0.2.1 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0 yarl-1.18.3\n"]}]},{"cell_type":"code","source":["import operator\n","from typing import Annotated, Any, Optional\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langgraph.graph import END, StateGraph\n","from pydantic import BaseModel, Field\n","\n","import os\n","from google.colab import userdata\n","\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n","os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n","os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n","os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\"\n","\n","\n","# ペルソナを表すデータモデル\n","class Persona(BaseModel):\n","    name: str = Field(..., description=\"ペルソナの名前\")\n","    background: str = Field(..., description=\"ペルソナの背景情報\")\n","\n","\n","# ペルソナのリストを表すデータモデル\n","class Personas(BaseModel):\n","    personas: list[Persona] = Field(\n","        default_factory=list, description=\"ペルソナのリスト\"\n","    )\n","\n","\n","# インタビュー内容を表すデータモデル\n","class Interview(BaseModel):\n","    persona: Persona = Field(..., description=\"インタビュー対象のペルソナ\")\n","    question: str = Field(..., description=\"インタビューでの質問\")\n","    answer: str = Field(..., description=\"インタビューでの回答\")\n","\n","\n","# インタビュー結果のリストを表すデータモデル\n","class InterviewResult(BaseModel):\n","    interviews: list[Interview] = Field(\n","        default_factory=list, description=\"インタビュー結果のリスト\"\n","    )\n","\n","\n","# エージェントの内部状態を表すモデル\n","class InterviewState(BaseModel):\n","    user_request: str = Field(..., description=\"ユーザーからのリクエスト\")\n","    personas: Annotated[list[Persona], operator.add] = Field(\n","        default_factory=list, description=\"生成されたペルソナのリスト\"\n","    )\n","    interviews: Annotated[list[Interview], operator.add] = Field(\n","        default_factory=list, description=\"実施されたインタビューのリスト\"\n","    )\n","    seo_strategy_doc: str = Field(default=\"\", description=\"生成されたSEO戦略ドキュメント\")\n","    iteration: int = Field(\n","        default=0, description=\"ペルソナ生成とインタビューの反復回数\"\n","    )\n","\n","\n","# ペルソナを生成するクラス（SEOに興味のあるユーザーを想定）\n","class PersonaGenerator:\n","    def __init__(self, llm: ChatOpenAI, k: int = 5):\n","        self.llm = llm.with_structured_output(Personas)\n","        self.k = k\n","\n","    def run(self, user_request: str) -> Personas:\n","        # --- SEO向けに変更したプロンプト ここから ---\n","\n","        prompt = ChatPromptTemplate.from_messages(\n","            [\n","                (\n","                    \"system\",\n","                    \"あなたは依頼元が提供するサービスに関心を持ち得る潜在顧客のペルソナ作成に特化した専門家です。\"\n","                    \"それぞれのペルソナが具体的にどんな悩みを抱え、どんな情報を求めているかを明確に示してください。\"\n","                ),\n","                (\n","                    \"human\",\n","                    (\n","                        f\"以下の内容で、{self.k}人の多様なペルソナを生成してください。\\n\\n\"\n","                        \"【サービスの概要】\\n{user_request}\\n\\n\"\n","                        \"各ペルソナには以下を含めてください:\\n\"\n","                        \"- 名前\\n\"\n","                        \"- 年齢、性別、職業\\n\"\n","                        \"- どんな課題・悩みを抱えているか\\n\"\n","                        \"- あなたのサービスを見つける可能性が高い情報源（検索エンジン、SNS、知人の紹介など）\\n\"\n","                        \"- どんな情報（価格、口コミ、機能比較など）を特に重視するか\\n\"\n","                        \"- どんなキーワードで検索しそうか（推定でOK）\\n\"\n","                        \"具体的でリアルに想像しやすい設定をお願いします。\"\n","                    ),\n","                ),\n","            ]\n","        )\n","        # --- SEO向けに変更したプロンプト ここまで ---\n","        chain = prompt | self.llm\n","        return chain.invoke({\"user_request\": user_request})\n","\n","\n","# インタビューを実施するクラス（SEOにおける悩みやニーズを深堀りする）\n","class InterviewConductor:\n","    def __init__(self, llm: ChatOpenAI):\n","        self.llm = llm\n","\n","    def run(self, user_request: str, personas: list[Persona]) -> InterviewResult:\n","        questions = self._generate_questions(user_request, personas)\n","        answers = self._generate_answers(personas, questions)\n","        interviews = self._create_interviews(personas, questions, answers)\n","        return InterviewResult(interviews=interviews)\n","\n","    def _generate_questions(\n","        self, user_request: str, personas: list[Persona]\n","    ) -> list[str]:\n","        # --- SEO向けの質問生成用プロンプト ---\n","        question_prompt = ChatPromptTemplate.from_messages(\n","            [\n","                (\n","                    \"system\",\n","                    \"あなたはインタビュアーです。潜在顧客の悩みや課題を深掘りし、\"\n","                    \"どのようにサービスを見つけ、選び、利用しようとしているのかを明らかにする質問を考えるプロです。\"\n","                ),\n","                (\n","                    \"human\",\n","                    (\n","                        \"以下の読者ペルソナが、あなたのサービスに対して抱えている悩みや、\"\n","                        \"検索やSNSを含む情報収集の実態を率直に話せるようなオープンな質問を作成してください。\\n\\n\"\n","                        \"【サービスの概要】\\n{user_request}\\n\\n\"\n","                        \"【読者ペルソナ】\\n{persona_name} - {persona_background}\\n\\n\"\n","                        \"質問のポイント:\\n\"\n","                        \"- 現在の悩みや目的を具体的に引き出せる\\n\"\n","                        \"- どこで情報収集しているかを明確にできる\\n\"\n","                        \"- 何を重視して比較検討しているのかを知る\\n\"\n","                        \"シンプルだが、回答者が深く考えられるようにしてください。\"\n","                    ),\n","                ),\n","            ]\n","        )\n","        question_chain = question_prompt | self.llm | StrOutputParser()\n","        question_queries = [\n","            {\n","                \"user_request\": user_request,\n","                \"persona_name\": persona.name,\n","                \"persona_background\": persona.background,\n","            }\n","            for persona in personas\n","        ]\n","        return question_chain.batch(question_queries)\n","\n","    def _generate_answers(\n","        self, personas: list[Persona], questions: list[str]\n","    ) -> list[str]:\n","        # --- SEO向けの回答生成用プロンプト ---\n","\n","        answer_prompt = ChatPromptTemplate.from_messages(\n","            [\n","                (\n","                    \"system\",\n","                    \"あなたは以下のペルソナです。サービスに関して抱えている具体的な悩み、\"\n","                    \"どのように情報を探すのか、何を基準に選ぶのかなどを正直に答えてください。\"\n","                ),\n","                (\n","                    \"human\",\n","                    (\n","                        \"ペルソナ: {persona_name} - {persona_background}\\n\\n\"\n","                        \"質問: {question}\\n\\n\"\n","                        \"回答のポイント:\\n\"\n","                        \"- どのような情報収集プロセスを踏んでいるか\\n\"\n","                        \"- 検索エンジンを使う場合、どんなキーワードを想定しているか\\n\"\n","                        \"- 何を重視しているか（価格、口コミ、評判、機能、サポート等）\\n\"\n","                        \"- 具体的な利用シーンや期待している効果\\n\"\n","                    ),\n","                ),\n","            ]\n","        )\n","\n","        answer_chain = answer_prompt | self.llm | StrOutputParser()\n","        answer_queries = [\n","            {\n","                \"persona_name\": persona.name,\n","                \"persona_background\": persona.background,\n","                \"question\": question,\n","            }\n","            for persona, question in zip(personas, questions)\n","        ]\n","        return answer_chain.batch(answer_queries)\n","\n","    def _create_interviews(\n","        self, personas: list[Persona], questions: list[str], answers: list[str]\n","    ) -> list[Interview]:\n","        return [\n","            Interview(persona=persona, question=q, answer=a)\n","            for persona, q, a in zip(personas, questions, answers)\n","        ]\n","\n","\n","# SEO戦略ドキュメントを生成するクラス\n","class SEOStrategyDocumentGenerator:\n","    def __init__(self, llm: ChatOpenAI):\n","        self.llm = llm\n","\n","    def run(self, user_request: str, interviews: list[Interview]) -> str:\n","        # --- SEO戦略立案のためのプロンプト ---\n","        prompt = ChatPromptTemplate.from_messages(\n","            [\n","                (\n","                    \"system\",\n","                    (\n","                        \"あなたはSEO戦略を立案するプロフェッショナルです。\"\n","                        \"以下の情報に基づいて、具体的で効果的なSEO戦略を提案してください。\"\n","                        \"キーワード調査やコンテンツの最適化、リンクビルディング、\"\n","                        \"技術的SEO（サイト速度・モバイル対応など）、競合分析など、\"\n","                        \"幅広い視点からアドバイスを行ってください。\"\n","                    ),\n","                ),\n","                (\n","                    \"human\",\n","                    (\n","                        \"以下のWebサイト概要と複数の読者ペルソナ（インタビュー結果）に基づいて、\"\n","                        \"実践的なSEO戦略ドキュメントを作成してください。\"\n","                        \"具体的なアクションプラン、優先度、ツールの活用方法、\"\n","                        \"必要に応じたリソースやコストの見積もりなども含めてください。\\n\\n\"\n","                        \"【Webサイトの概要】\\n{user_request}\\n\\n\"\n","                        \"【インタビュー結果】\\n{interview_results}\\n\\n\"\n","                        \"最低限、以下の項目を盛り込んでください:\\n\"\n","                        \"1. SEO施策の目的（どんなKPIを達成したいか）\\n\"\n","                        \"2. ターゲット層と主要キーワード\\n\"\n","                        \"3. 現状の課題（技術的SEO、コンテンツ、バックリンクなど）\\n\"\n","                        \"4. 施策の優先度とロードマップ（短期・中期・長期）\\n\"\n","                        \"5. キーワード戦略やコンテンツ最適化の方針\\n\"\n","                        \"6. リンクビルディングの戦略\\n\"\n","                        \"7. 競合サイトの分析ポイント\\n\"\n","                        \"8. 必要なツールやリソース、運用コストの目安\\n\"\n","                        \"9. モニタリングと改善サイクル\\n\\n\"\n","                        \"以上を踏まえ、プロが実践できるレベルの詳細な戦略ドキュメントを作成してください。\"\n","                    ),\n","                ),\n","            ]\n","        )\n","        chain = prompt | self.llm | StrOutputParser()\n","\n","        # インタビュー結果をテキスト形式にまとめる\n","        interview_results_text = \"\\n\".join(\n","            f\"ペルソナ: {i.persona.name} - {i.persona.background}\\n\"\n","            f\"質問: {i.question}\\n回答: {i.answer}\\n\"\n","            for i in interviews\n","        )\n","\n","        return chain.invoke(\n","            {\n","                \"user_request\": user_request,\n","                \"interview_results\": interview_results_text,\n","            }\n","        )\n","\n","\n","# 「SEO戦略立案AIエージェント」のクラス\n","class SEOAgent:\n","    def __init__(self, llm: ChatOpenAI, k: Optional[int] = None):\n","        self.persona_generator = PersonaGenerator(llm=llm, k=k)\n","        self.interview_conductor = InterviewConductor(llm=llm)\n","        self.seo_strategy_generator = SEOStrategyDocumentGenerator(llm=llm)\n","        self.graph = self._create_graph()\n","\n","    def _create_graph(self) -> StateGraph:\n","        workflow = StateGraph(InterviewState)\n","\n","        # ノードを追加\n","        workflow.add_node(\"generate_personas\", self._generate_personas)\n","        workflow.add_node(\"conduct_interviews\", self._conduct_interviews)\n","        workflow.add_node(\"generate_strategy\", self._generate_strategy)\n","\n","        # エントリーポイント\n","        workflow.set_entry_point(\"generate_personas\")\n","\n","        # 遷移設定\n","        workflow.add_edge(\"generate_personas\", \"conduct_interviews\")\n","        workflow.add_edge(\"conduct_interviews\", \"generate_strategy\")\n","        workflow.add_edge(\"generate_strategy\", END)\n","\n","        return workflow.compile()\n","\n","    def _generate_personas(self, state: InterviewState) -> dict[str, Any]:\n","        new_personas = self.persona_generator.run(state.user_request)\n","        return {\n","            \"personas\": new_personas.personas,\n","            \"iteration\": state.iteration + 1,\n","        }\n","\n","    def _conduct_interviews(self, state: InterviewState) -> dict[str, Any]:\n","        # ペルソナ数が多い場合、最後の5人のみに制限\n","        new_personas = state.personas[-5:]\n","        new_interviews = self.interview_conductor.run(\n","            state.user_request, new_personas\n","        )\n","        return {\"interviews\": new_interviews.interviews}\n","\n","    def _generate_strategy(self, state: InterviewState) -> dict[str, Any]:\n","        seo_strategy_doc = self.seo_strategy_generator.run(\n","            state.user_request, state.interviews\n","        )\n","        return {\"seo_strategy_doc\": seo_strategy_doc}\n","\n","    def run(self, user_request: str) -> str:\n","        initial_state = InterviewState(user_request=user_request)\n","        final_state = self.graph.invoke(initial_state)\n","        return final_state[\"seo_strategy_doc\"]\n","\n","\n","def main():\n","    user_request = input(\"SEO戦略を立案したいWebサイトの概要や目的を入力してください: \")\n","    k = 3  # 生成するペルソナの人数（必要に応じて変更可能）\n","\n","    # モデル名は利用できるものに合わせて変更してください\n","    llm = ChatOpenAI(model_name=\"gpt-4o-2024-11-20\", temperature=0.0)\n","    agent = SEOAgent(llm=llm, k=k)\n","    final_output = agent.run(user_request=user_request)\n","\n","    print(final_output)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcOz1fTNtTKX","executionInfo":{"status":"ok","timestamp":1736131817196,"user_tz":-540,"elapsed":101163,"user":{"displayName":"Shohei O","userId":"12040440607234237913"}},"outputId":"92a7db5c-04ca-4466-a6f0-39fea2c2da8d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["SEO戦略を立案したいWebサイトの概要や目的を入力してください: 不動産業界向けの財務モデリング、Excelテンプレートの標準化・自動化、トレーニングなどの提供\n","# SEO戦略ドキュメント: 不動産業界向け財務モデリング・Excelテンプレート提供サイト\n","\n","---\n","\n","## 1. **SEO施策の目的**\n","### **目的**\n","- **リード獲得**: 不動産業界の財務部門や投資家、コンサルタントをターゲットに、問い合わせや資料ダウンロード、トライアル申し込みを増加させる。\n","- **トラフィック増加**: 月間オーガニックトラフィックを現状の2倍に増加。\n","- **コンバージョン率向上**: サイト訪問者のうち、問い合わせや資料請求に至るコンバージョン率を5%以上に向上。\n","\n","### **KPI**\n","- 月間オーガニックトラフィック: **+100%増加**（例: 5,000→10,000セッション）\n","- リード獲得数: **月間50件以上**\n","- コンバージョン率: **5%以上**\n","- 主要キーワードでの検索順位: **上位3位以内を10個以上達成**\n","\n","---\n","\n","## 2. **ターゲット層と主要キーワード**\n","### **ターゲット層**\n","1. **田中一郎（不動産会社の財務部長）**\n","   - ニーズ: 財務モデリングの効率化、エラー削減、標準化\n","   - 情報収集方法: Google検索、業界セミナー、LinkedIn\n","   - 重視ポイント: 導入のしやすさ、既存Excelとの連携、コストパフォーマンス\n","\n","2. **佐藤美咲（不動産投資家）**\n","   - ニーズ: 初心者でも使いやすい財務分析ツール、学習リソース\n","   - 情報収集方法: Google検索、YouTube、SNS\n","   - 重視ポイント: 価格、使いやすさ、初心者向けサポート\n","\n","3. **山本健太（不動産コンサルタント）**\n","   - ニーズ: クライアント向けレポート作成の効率化、データの自動化\n","   - 情報収集方法: 業界セミナー、Google検索、LinkedIn\n","   - 重視ポイント: カスタマイズ性、サポート体制、実績\n","\n","### **主要キーワード**\n","#### **短期（競合性低・ニッチ）**\n","- 「不動産 財務モデリング テンプレート」\n","- 「Excel 自動化 不動産」\n","- 「不動産 キャッシュフロー計算」\n","- 「財務モデリング 初心者」\n","\n","#### **中期（競合性中）**\n","- 「不動産業務 効率化ツール」\n","- 「Excel 財務分析 テンプレート」\n","- 「不動産投資 ROI 計算」\n","- 「財務モデリング トレーニング」\n","\n","#### **長期（競合性高・検索ボリューム大）**\n","- 「不動産 財務分析」\n","- 「Excel 自動化」\n","- 「財務モデリング ツール」\n","- 「不動産投資 効率化」\n","\n","---\n","\n","## 3. **現状の課題**\n","### **技術的SEO**\n","- **サイト速度**: ページ読み込み速度が遅い（特にモバイル）。Core Web Vitalsのスコアが低い。\n","- **モバイル対応**: レスポンシブデザインが不完全で、モバイルでのユーザー体験が悪い。\n","- **インデックス最適化**: サイトマップが不完全で、重要なページがGoogleにインデックスされていない。\n","- **内部リンク**: 内部リンク構造が弱く、関連コンテンツ間のリンクが不足。\n","\n","### **コンテンツ**\n","- **キーワード最適化不足**: 主要キーワードがタイトルタグやメタディスクリプションに適切に含まれていない。\n","- **コンテンツの深さ**: ページ内容が浅く、専門性や権威性が不足。\n","- **CTA（行動喚起）不足**: 問い合わせや資料請求への誘導が弱い。\n","\n","### **バックリンク**\n","- **質の高いリンク不足**: 業界関連サイトや信頼性の高いドメインからのリンクが少ない。\n","- **リンクプロファイルの偏り**: 自然なアンカーテキストの多様性が不足。\n","\n","---\n","\n","## 4. **施策の優先度とロードマップ**\n","### **短期（1～3ヶ月）**\n","1. **技術的SEOの改善**\n","   - サイト速度の最適化（画像圧縮、キャッシュ設定、CDN導入）\n","   - モバイル対応の強化（レスポンシブデザインの修正）\n","   - サイトマップとrobots.txtの最適化\n","   - 内部リンク構造の強化\n","\n","2. **キーワード戦略の実行**\n","   - 主要キーワードを含むタイトルタグ、メタディスクリプション、H1タグの最適化\n","   - 既存コンテンツのリライト（キーワード密度の調整、CTA追加）\n","\n","3. **コンテンツの拡充**\n","   - 「不動産財務モデリングの基本」などの初心者向けブログ記事を作成\n","   - ペルソナごとに特化したランディングページを作成\n","\n","### **中期（4～6ヶ月）**\n","1. **コンテンツマーケティング**\n","   - 「不動産業界の財務モデリング成功事例」などのケーススタディ記事を作成\n","   - 動画コンテンツ（YouTube）でのExcelチュートリアル配信\n","\n","2. **リンクビルディング**\n","   - 業界関連サイトやブログへのゲスト投稿\n","   - 業界セミナーやイベントでの資料提供を通じたリンク獲得\n","   - 不動産関連フォーラムやコミュニティでの積極的な情報発信\n","\n","3. **競合分析**\n","   - 競合サイトのコンテンツとバックリンクを分析し、ギャップを埋める施策を実行\n","\n","### **長期（7～12ヶ月）**\n","1. **エキスパートコンテンツの作成**\n","   - 「不動産財務モデリングのトレンド」などの専門性の高いホワイトペーパーを作成\n","   - 業界リーダーとのインタビュー記事や共同コンテンツの作成\n","\n","2. **ブランド認知の向上**\n","   - 業界イベントでのスポンサーシップや講演\n","   - LinkedIn広告やリターゲティング広告を活用した認知拡大\n","\n","3. **継続的な改善**\n","   - 定期的なSEOパフォーマンスのモニタリングと改善\n","   - 新しいキーワードの発掘とコンテンツの追加\n","\n","---\n","\n","## 5. **キーワード戦略やコンテンツ最適化の方針**\n","- **キーワードの選定**: 検索ボリューム、競合性、ターゲット層のニーズを基に選定。\n","- **コンテンツの形式**:\n","  - 初心者向けガイド（例: 「不動産財務モデリングの基本」）\n","  - 実践的なテンプレート紹介（例: 「無料ダウンロード: 不動産キャッシュフローテンプレート」）\n","  - ケーススタディ（例: 「導入事例: 財務モデリングで業務効率化を実現」）\n","- **CTAの強化**: 各ページに明確な行動喚起（例: 「無料トライアルを試す」「テンプレートをダウンロード」）。\n","\n","---\n","\n","## 6. **リンクビルディングの戦略**\n","- **業界サイトとの提携**: 不動産関連メディアやブログへの寄稿。\n","- **リソースページの活用**: 「不動産業界向けの便利ツール集」などのリソースページを作成し、リンクを獲得。\n","- **インフルエンサーとの連携**: 不動産投資家やコンサルタントのインフルエンサーにツールを提供し、レビューやリンクを獲得。\n","\n","---\n","\n","## 7. **競合サイトの分析ポイント**\n","- **競合サイト例**: \n","  - 「不動産財務モデリングツール」提供サイト\n","  - 「Excelテンプレート」配布サイト\n","- **分析項目**:\n","  - トップページとランディングページの構造\n","  - 使用しているキーワードとコンテンツの深さ\n","  - バックリンクの質と量（AhrefsやSEMrushを活用）\n","  - トラフィックソース（オーガニック、リファラル、ソーシャル）\n","\n","---\n","\n","## 8. **必要なツールやリソース、運用コストの目安**\n","### **ツール**\n","- **キーワード調査**: Ahrefs、SEMrush、Googleキーワードプランナー\n","- **技術的SEO**: Google Search Console、PageSpeed Insights、Screaming Frog\n","- **コンテンツ管理**: WordPress、Grammarly\n","- **リンクビルディング**: BuzzStream、Hunter.io\n","- **モニタリング**: Google Analytics、Hotjar\n","\n","### **リソース**\n","- **人員**: SEO担当者1名、コンテンツライター1～2名、技術担当1名\n","- **コスト目安**: 月額20～30万円（ツール費用、外注費用含む）\n","\n","---\n","\n","## 9. **モニタリングと改善サイクル**\n","1. **モニタリング**\n","   - 毎月: トラフィック、検索順位、コンバージョン率をGoogle AnalyticsとSearch Consoleで確認。\n","   - 四半期: バックリンクの増加状況をAhrefsで確認。\n","\n","2. **改善サイクル**\n","   - データ分析に基づき、低パフォーマンスのページをリライト。\n","   - 新しいキーワードを追加し、コンテンツを拡充。\n","   - 技術的SEOの定期的な監査と修正。\n","\n","---\n","\n","以上の戦略を実行することで、ターゲット層にリーチし、トラフィックとコンバージョンを最大化することが可能です。\n"]}]},{"cell_type":"markdown","source":["# English Version"],"metadata":{"id":"P7k75yEWvmZB"}}]}