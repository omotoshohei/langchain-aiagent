{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.0 langchain-openai==0.2.0 langgraph==0.2.22 httpx==0.27.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0qYIfSkXLuC",
        "outputId": "0cb5bfd6-5ebe-42bf-8ca6-ac54cd04f4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.0\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-openai==0.2.0\n",
            "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langgraph==0.2.22\n",
            "  Downloading langgraph-0.2.22-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting httpx==0.27.2\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (6.0.2)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.3.0)\n",
            "  Downloading SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.3.0)\n",
            "  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.3.0)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain==0.3.0)\n",
            "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.0)\n",
            "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.0)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.10.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.3.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai==0.2.0)\n",
            "  Downloading openai-1.59.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.0)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langgraph-checkpoint<2.0.0,>=1.0.2 (from langgraph==0.2.22)\n",
            "  Downloading langgraph_checkpoint-1.0.12-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (2024.12.14)\n",
            "Collecting httpcore==1.* (from httpx==0.27.2)\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx==0.27.2)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n",
            "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (24.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n",
            "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n",
            "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n",
            "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0)\n",
            "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<2.0.0,>=1.0.2->langgraph==0.2.22) (1.1.0)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0)\n",
            "  Downloading orjson-3.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0)\n",
            "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (4.67.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx==0.27.2) (1.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (2.3.0)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.3.0)\n",
            "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.0) (2024.11.6)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain==0.3.0)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.2.22-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
            "Downloading langgraph_checkpoint-1.0.12-py3-none-any.whl (17 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.59.6-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.4/130.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tenacity, propcache, orjson, multidict, jsonpointer, jiter, h11, greenlet, frozenlist, async-timeout, aiohappyeyeballs, yarl, tiktoken, SQLAlchemy, requests-toolbelt, jsonpatch, httpcore, aiosignal, httpx, aiohttp, openai, langsmith, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 async-timeout-4.0.3 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.27.2 jiter-0.8.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-core-0.3.29 langchain-openai-0.2.0 langchain-text-splitters-0.3.5 langgraph-0.2.22 langgraph-checkpoint-1.0.12 langsmith-0.1.147 multidict-6.1.0 openai-1.59.6 orjson-3.10.14 propcache-0.2.1 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0 yarl-1.18.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF8IYG3AXEMR",
        "outputId": "5d225601-7530-4791-c8e8-2ffe635a64c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "運用したいSNSアカウントについて記載してください: 副業ですが、東京でSEOや広告、デジタルマーケティングの業務支援サービスを販売をしたいです。 Python、データサイエンス、AI開発が得意なので、その強みを活かしたいです。あと英語ができるので、東京にいる外国人相手にビジネスがしたいです。\n",
            "### SNSアカウント運用マニュアル\n",
            "\n",
            "---\n",
            "\n",
            "#### 1. SNS運用の目的\n",
            "- **副業としてのデジタルマーケティング業務支援サービスの認知拡大と顧客獲得**\n",
            "  - SEO、広告運用、データサイエンス、AI開発のスキルを活かし、東京を拠点に日本人および外国人向けにサービスを提供。\n",
            "  - ターゲット読者の悩みを解決する情報を発信し、信頼を構築。\n",
            "  - 自身の専門性をアピールし、クライアント獲得につなげる。\n",
            "\n",
            "---\n",
            "\n",
            "#### 2. ターゲット読者\n",
            "1. **田中翔太（32歳、日本人男性）**\n",
            "   - 中小企業のマーケティング担当者。\n",
            "   - SEOや広告の基礎知識はあるが、データサイエンスやAIは初心者。\n",
            "   - 副業に興味があり、スキルアップを目指している。\n",
            "\n",
            "2. **エミリー・ジョンソン（28歳、アメリカ人女性）**\n",
            "   - 東京在住の英語教師。\n",
            "   - デジタルマーケティングやPython、AIに興味を持ち始めた初心者。\n",
            "   - 英語を活かして副業を始めたいと考えている。\n",
            "\n",
            "3. **佐藤美咲（40歳、日本人女性）**\n",
            "   - フリーランスのデータアナリスト。\n",
            "   - データサイエンスやPythonに精通しているが、SEOや広告の知識は浅い。\n",
            "   - 外国人向けビジネスに挑戦したいと考えている。\n",
            "\n",
            "---\n",
            "\n",
            "#### 3. ターゲット読者の悩み\n",
            "- **田中翔太**\n",
            "  - 自分のスキルを副業にどう結びつけるか分からない。\n",
            "  - データサイエンスやAIの学習方法が分からない。\n",
            "  - クライアント獲得や営業に不安がある。\n",
            "\n",
            "- **エミリー・ジョンソン**\n",
            "  - デジタルマーケティングやPython、AIの初心者で、どこから始めればいいか分からない。\n",
            "  - 英語を活かした副業の具体的なイメージが湧かない。\n",
            "  - 限られた時間で効率的に学びたい。\n",
            "\n",
            "- **佐藤美咲**\n",
            "  - SEOや広告の基礎知識が不足している。\n",
            "  - 外国人向けビジネスの具体的な戦略が分からない。\n",
            "  - リソースが限られている中で、どの分野に優先的に取り組むべきか迷っている。\n",
            "\n",
            "---\n",
            "\n",
            "#### 4. トピックとハッシュタグ\n",
            "- **トピック**\n",
            "  - デジタルマーケティングの基礎知識（SEO、広告運用、データ分析）。\n",
            "  - PythonやAIを活用したマーケティングの効率化。\n",
            "  - 副業の始め方やクライアント獲得のコツ。\n",
            "  - 外国人向けビジネスの成功事例や文化的なポイント。\n",
            "  - 英語を活かしたマーケティングの可能性。\n",
            "\n",
            "- **ハッシュタグ**\n",
            "  - #デジタルマーケティング\n",
            "  - #SEO\n",
            "  - #副業\n",
            "  - #Python\n",
            "  - #AI\n",
            "  - #データサイエンス\n",
            "  - #東京ビジネス\n",
            "  - #外国人向けビジネス\n",
            "  - #英語で仕事\n",
            "  - #マーケティング初心者\n",
            "\n",
            "---\n",
            "\n",
            "#### 5. 運用するSNSプラットフォームと投稿頻度、タイミング、投稿形式\n",
            "- **プラットフォーム**\n",
            "  1. **Twitter（X）**\n",
            "     - 投稿頻度: 1日1～2回。\n",
            "     - タイミング: 朝8時～10時、夜8時～10時。\n",
            "     - 投稿形式: テキスト、画像付き投稿、リンクシェア。\n",
            "\n",
            "  2. **Instagram**\n",
            "     - 投稿頻度: 週3～4回。\n",
            "     - タイミング: 夜8時～10時。\n",
            "     - 投稿形式: 画像、動画（リール）、ストーリー。\n",
            "\n",
            "  3. **LinkedIn**\n",
            "     - 投稿頻度: 週2～3回。\n",
            "     - タイミング: 平日昼12時～午後2時。\n",
            "     - 投稿形式: テキスト、画像、記事リンク。\n",
            "\n",
            "  4. **YouTube**\n",
            "     - 投稿頻度: 月2～4回。\n",
            "     - タイミング: 土日午前10時～午後2時。\n",
            "     - 投稿形式: 動画（5～10分の解説動画、チュートリアル）。\n",
            "\n",
            "- **投稿形式**\n",
            "  - **テキスト**: 短いアドバイスやトピックの紹介。\n",
            "  - **画像**: インフォグラフィックや図解。\n",
            "  - **動画**: チュートリアルや実践例。\n",
            "  - **ストーリー**: 日常の活動や簡単なQ&A。\n",
            "  - **ライブ配信**: 月1回、フォロワーとの交流や質問回答。\n",
            "\n",
            "---\n",
            "\n",
            "#### 6. 投稿内容のテーマ\n",
            "1. **初心者向け解説**\n",
            "   - SEOや広告の基礎知識。\n",
            "   - PythonやAIの簡単な活用方法。\n",
            "   - 副業の始め方やスキルの活かし方。\n",
            "\n",
            "2. **実践的なアドバイス**\n",
            "   - Googleアナリティクスのデータ活用法。\n",
            "   - AIツールを使ったマーケティング効率化。\n",
            "   - クライアント獲得のための営業・ネットワーキングのコツ。\n",
            "\n",
            "3. **成功事例の紹介**\n",
            "   - デジタルマーケティングで成果を上げた事例。\n",
            "   - 外国人向けビジネスの成功例。\n",
            "   - 英語を活かした副業の具体例。\n",
            "\n",
            "4. **ターゲット別の情報発信**\n",
            "   - 日本人向け: 副業の始め方、SEOや広告の基礎。\n",
            "   - 外国人向け: 日本市場でのマーケティングのポイント。\n",
            "\n",
            "5. **インタラクティブなコンテンツ**\n",
            "   - フォロワーからの質問に答えるQ&A。\n",
            "   - 簡単なクイズやアンケート。\n",
            "\n",
            "---\n",
            "\n",
            "#### 7. 注意事項\n",
            "1. **一貫性のあるブランディング**\n",
            "   - 投稿のトーンやデザインを統一。\n",
            "   - プロフェッショナルでありながら親しみやすい印象を心がける。\n",
            "\n",
            "2. **ターゲット読者に合わせた言語選択**\n",
            "   - 日本人向け投稿は日本語、外国人向け投稿は英語で発信。\n",
            "   - 両方の読者に響く内容の場合はバイリンガル投稿を検討。\n",
            "\n",
            "3. **過度な専門用語の使用を避ける**\n",
            "   - 初心者にも分かりやすい言葉で説明。\n",
            "   - 必要に応じて用語の簡単な解説を添える。\n",
            "\n",
            "4. **著作権やプライバシーに配慮**\n",
            "   - 他人のコンテンツを使用する際は必ず許可を得る。\n",
            "   - クライアント事例を紹介する場合は匿名化や許可を取る。\n",
            "\n",
            "5. **定期的な分析と改善**\n",
            "   - SNSのインサイト機能を活用し、投稿の効果を分析。\n",
            "   - フォロワーの反応をもとにコンテンツを改善。\n",
            "\n",
            "---\n",
            "\n",
            "以上の運用マニュアルをもとに、ターゲット読者の悩みを解決し、信頼を構築するSNSアカウント運営を行ってください。\n"
          ]
        }
      ],
      "source": [
        "import operator\n",
        "from typing import Annotated, Any, Optional\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import END, StateGraph\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\"\n",
        "\n",
        "\n",
        "# ペルソナを表すデータモデル\n",
        "class Persona(BaseModel):\n",
        "    name: str = Field(..., description=\"ペルソナの名前\")\n",
        "    background: str = Field(..., description=\"ペルソナの持つ背景\")\n",
        "\n",
        "\n",
        "# ペルソナのリストを表すデータモデル\n",
        "class Personas(BaseModel):\n",
        "    personas: list[Persona] = Field(\n",
        "        default_factory=list, description=\"ペルソナのリスト\"\n",
        "    )\n",
        "\n",
        "\n",
        "# インタビュー内容を表すデータモデル\n",
        "class Interview(BaseModel):\n",
        "    persona: Persona = Field(..., description=\"インタビュー対象のペルソナ\")\n",
        "    question: str = Field(..., description=\"インタビューでの質問\")\n",
        "    answer: str = Field(..., description=\"インタビューでの回答\")\n",
        "\n",
        "\n",
        "# インタビュー結果のリストを表すデータモデル\n",
        "class InterviewResult(BaseModel):\n",
        "    interviews: list[Interview] = Field(\n",
        "        default_factory=list, description=\"インタビュー結果のリスト\"\n",
        "    )\n",
        "\n",
        "\n",
        "# （情報評価用の EvaluationResult クラスは削除）\n",
        "\n",
        "\n",
        "# 要件定義生成AIエージェントのステート\n",
        "class InterviewState(BaseModel):\n",
        "    user_request: str = Field(..., description=\"ユーザーからのリクエスト\")\n",
        "    personas: Annotated[list[Persona], operator.add] = Field(\n",
        "        default_factory=list, description=\"生成されたペルソナのリスト\"\n",
        "    )\n",
        "    interviews: Annotated[list[Interview], operator.add] = Field(\n",
        "        default_factory=list, description=\"実施されたインタビューのリスト\"\n",
        "    )\n",
        "    requirements_doc: str = Field(default=\"\", description=\"生成された要件定義\")\n",
        "    iteration: int = Field(\n",
        "        default=0, description=\"ペルソナ生成とインタビューの反復回数\"\n",
        "    )\n",
        "    # ↓ 情報評価ステップ削除に伴い、このフラグも削除\n",
        "    # is_information_sufficient: bool = Field(\n",
        "    #     default=False, description=\"情報が十分かどうか\"\n",
        "    # )\n",
        "\n",
        "\n",
        "# ペルソナを生成するクラス\n",
        "class PersonaGenerator:\n",
        "    def __init__(self, llm: ChatOpenAI, k: int = 5):\n",
        "        self.llm = llm.with_structured_output(Personas)\n",
        "        self.k = k\n",
        "\n",
        "    def run(self, user_request: str) -> Personas:\n",
        "        # プロンプトテンプレートを定義\n",
        "        prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"あなたはSNSアカウントのターゲットユーザーへのインタビュー用の多様なペルソナを作成する専門家です。\",\n",
        "                ),\n",
        "                (\n",
        "                    \"human\",\n",
        "                    f\"以下のSNSのトピックに関するインタビュー用に、{self.k}人の多様なペルソナを生成してください。\\n\\n\"\n",
        "                    \"トピック: {user_request}\\n\\n\"\n",
        "                    \"各読者ペルソナには名前と簡単な背景を含めてください。年齢、性別、職業、トピックに対する知識レベルにおいて多様性を確保してください。\",\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        chain = prompt | self.llm\n",
        "        return chain.invoke({\"user_request\": user_request})\n",
        "\n",
        "\n",
        "# インタビューを実施するクラス\n",
        "class InterviewConductor:\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        self.llm = llm\n",
        "\n",
        "    def run(self, user_request: str, personas: list[Persona]) -> InterviewResult:\n",
        "        # 質問を生成\n",
        "        questions = self._generate_questions(\n",
        "            user_request=user_request, personas=personas\n",
        "        )\n",
        "        # 回答を生成\n",
        "        answers = self._generate_answers(personas=personas, questions=questions)\n",
        "        # 組み合わせからインタビューリストを作成\n",
        "        interviews = self._create_interviews(personas, questions, answers)\n",
        "        return InterviewResult(interviews=interviews)\n",
        "\n",
        "    def _generate_questions(\n",
        "        self, user_request: str, personas: list[Persona]\n",
        "    ) -> list[str]:\n",
        "        question_prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"あなたはインタビュアーです。ペルソナの悩みや課題を引き出すための質問を作成します。\",\n",
        "                ),\n",
        "                (\n",
        "                    \"human\",\n",
        "                    \"以下の読者ペルソナが、ブログ記事のトピックに関して自身の悩みや課題を話すための、オープンな質問を一つ作成してください。\\n\\n\"\n",
        "                    \"トピック: {user_request}\\n\"\n",
        "                    \"読者ペルソナ: {persona_name} - {persona_background}\\n\\n\"\n",
        "                    \"質問はシンプルで、このペルソナが自分の悩みを率直に話せるようにしてください。\",\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        question_chain = question_prompt | self.llm | StrOutputParser()\n",
        "        question_queries = [\n",
        "            {\n",
        "                \"user_request\": user_request,\n",
        "                \"persona_name\": persona.name,\n",
        "                \"persona_background\": persona.background,\n",
        "            }\n",
        "            for persona in personas\n",
        "        ]\n",
        "        return question_chain.batch(question_queries)\n",
        "\n",
        "    def _generate_answers(\n",
        "        self, personas: list[Persona], questions: list[str]\n",
        "    ) -> list[str]:\n",
        "        answer_prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"あなたは以下の読者ペルソナです。インタビュアーの質問に対して、あなたが抱えている悩みや課題を具体的に教えてください。\\n\\nペルソナ:  {persona_name} - {persona_background}\",\n",
        "                ),\n",
        "                (\"human\", \"質問: {question}\"),\n",
        "            ]\n",
        "        )\n",
        "        answer_chain = answer_prompt | self.llm | StrOutputParser()\n",
        "        answer_queries = [\n",
        "            {\n",
        "                \"persona_name\": persona.name,\n",
        "                \"persona_background\": persona.background,\n",
        "                \"question\": question,\n",
        "            }\n",
        "            for persona, question in zip(personas, questions)\n",
        "        ]\n",
        "        return answer_chain.batch(answer_queries)\n",
        "\n",
        "    def _create_interviews(\n",
        "        self, personas: list[Persona], questions: list[str], answers: list[str]\n",
        "    ) -> list[Interview]:\n",
        "        return [\n",
        "            Interview(persona=persona, question=q, answer=a)\n",
        "            for persona, q, a in zip(personas, questions, answers)\n",
        "        ]\n",
        "\n",
        "\n",
        "# （情報評価クラス InformationEvaluator は削除）\n",
        "\n",
        "\n",
        "# 要件定義書を生成するクラス\n",
        "class RequirementsDocumentGenerator:\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        self.llm = llm\n",
        "\n",
        "    def run(self, user_request: str, interviews: list[Interview]) -> str:\n",
        "        prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"あなたは収集した情報に基づいてSNSアカウントの運用マニュアルを作成する専門家です。\",\n",
        "                ),\n",
        "                (\n",
        "                    \"human\",\n",
        "                    \"以下のトピックと複数の読者ペルソナからのインタビュー結果に基づいて、SNSアカウントの運用マニュアルの指示書を作成してください。\\n\\n\"\n",
        "                    \"トピック: {user_request}\\n\\n\"\n",
        "                    \"インタビュー結果:\\n{interview_results}\\n\"\n",
        "                    \"運用マニュアルには以下のセクションを含めてください:\\n\"\n",
        "                    \"1. SNS運用の目的\\n\"\n",
        "                    \"2. ターゲット読者\\n\"\n",
        "                    \"3. ターゲット読者の悩み\\n\"\n",
        "                    \"4. トピックとハッシュタグ\\n\"\n",
        "                    \"5. 運用するSNSプラットフォームと投稿頻度、タイミング、投稿形式（テキスト、画像、動画、ストーリー、ライブ配信など）\\n\"\n",
        "                    \"6. 投稿内容のテーマ\\n\"\n",
        "                    \"7. 注意事項\\n\"\n",
        "                    \"出力は必ず日本語でお願いします。\\n\\n記事作成の指示書:\",\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        chain = prompt | self.llm | StrOutputParser()\n",
        "\n",
        "        # インタビュー結果をテキスト形式にまとめる\n",
        "        interview_results_text = \"\\n\".join(\n",
        "            f\"ペルソナ: {i.persona.name} - {i.persona.background}\\n\"\n",
        "            f\"質問: {i.question}\\n回答: {i.answer}\\n\"\n",
        "            for i in interviews\n",
        "        )\n",
        "\n",
        "        return chain.invoke(\n",
        "            {\n",
        "                \"user_request\": user_request,\n",
        "                \"interview_results\": interview_results_text,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "# 要件定義書生成AIエージェントのクラス\n",
        "class DocumentationAgent:\n",
        "    def __init__(self, llm: ChatOpenAI, k: Optional[int] = None):\n",
        "        self.persona_generator = PersonaGenerator(llm=llm, k=k)\n",
        "        self.interview_conductor = InterviewConductor(llm=llm)\n",
        "        # ↓ 情報評価クラスを削除したのでここも削除\n",
        "        # self.information_evaluator = InformationEvaluator(llm=llm)\n",
        "        self.requirements_generator = RequirementsDocumentGenerator(llm=llm)\n",
        "        self.graph = self._create_graph()\n",
        "\n",
        "    def _create_graph(self) -> StateGraph:\n",
        "        workflow = StateGraph(InterviewState)\n",
        "\n",
        "        # ノード追加（情報評価工程は入れない）\n",
        "        workflow.add_node(\"generate_personas\", self._generate_personas)\n",
        "        workflow.add_node(\"conduct_interviews\", self._conduct_interviews)\n",
        "        workflow.add_node(\"generate_requirements\", self._generate_requirements)\n",
        "\n",
        "        # エントリーポイント\n",
        "        workflow.set_entry_point(\"generate_personas\")\n",
        "\n",
        "        # 遷移設定\n",
        "        workflow.add_edge(\"generate_personas\", \"conduct_interviews\")\n",
        "        # 評価をスキップし直接要件定義書生成へ\n",
        "        workflow.add_edge(\"conduct_interviews\", \"generate_requirements\")\n",
        "\n",
        "        workflow.add_edge(\"generate_requirements\", END)\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def _generate_personas(self, state: InterviewState) -> dict[str, Any]:\n",
        "        new_personas: Personas = self.persona_generator.run(state.user_request)\n",
        "        return {\n",
        "            \"personas\": new_personas.personas,\n",
        "            \"iteration\": state.iteration + 1,\n",
        "        }\n",
        "\n",
        "    def _conduct_interviews(self, state: InterviewState) -> dict[str, Any]:\n",
        "        # ペルソナが多い場合は最後の5人のみに絞る\n",
        "        new_personas = state.personas[-5:]\n",
        "        new_interviews: InterviewResult = self.interview_conductor.run(\n",
        "            state.user_request, new_personas\n",
        "        )\n",
        "        return {\"interviews\": new_interviews.interviews}\n",
        "\n",
        "    # ↓ 情報評価は削除\n",
        "    # def _evaluate_information(self, state: InterviewState) -> dict[str, Any]:\n",
        "    #     ...\n",
        "\n",
        "    def _generate_requirements(self, state: InterviewState) -> dict[str, Any]:\n",
        "        requirements_doc: str = self.requirements_generator.run(\n",
        "            state.user_request, state.interviews\n",
        "        )\n",
        "        return {\"requirements_doc\": requirements_doc}\n",
        "\n",
        "    def run(self, user_request: str) -> str:\n",
        "        initial_state = InterviewState(user_request=user_request)\n",
        "        final_state = self.graph.invoke(initial_state)\n",
        "        return final_state[\"requirements_doc\"]\n",
        "\n",
        "\n",
        "# メイン関数\n",
        "def main():\n",
        "    user_request = input(\"運用したいSNSアカウントについて記載してください: \")\n",
        "    k = 3  # ペルソナの人数（必要に応じて変更可能）\n",
        "\n",
        "    llm = ChatOpenAI(model_name=\"gpt-4o-2024-11-20\", temperature=0.0)\n",
        "    # llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "    agent = DocumentationAgent(llm=llm, k=k)\n",
        "    final_output = agent.run(user_request=user_request)\n",
        "\n",
        "    print(final_output)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}